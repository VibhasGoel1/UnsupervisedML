{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX8lefEIIUhh"
   },
   "source": [
    "# IS 820 Homework 1\n",
    "\n",
    "Group Member Names:\n",
    "\n",
    "Reminder: you should not be sharing code across groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIo4YlN5IUhk"
   },
   "source": [
    "**Warning: make sure your compute environment is set up first.** This homework assignment assumes that you have already installed Anaconda Python 3 and spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMJ1uXgqIUhl"
   },
   "source": [
    "### Instructions\n",
    "1. Fill in group name, and member names above\n",
    "2. Fill in the code/text blocks to answer each question.\n",
    "3. Do *not* change any of the existing code provided.\n",
    "4. Run the entire notebook *before* submitting it on blackboard to make sure that the code actually runs without errors. (**Important**: Any code cells that you have entered code for but did not actually execute will be disregarded, so please be sure to actually run your code first and make sure it runs without errors! We may re-run a subset of your code for grading purposes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GP6xzMQIUhl"
   },
   "source": [
    "### [Problem 1] Basic Text Analysis [45 pts]\n",
    "\n",
    "This problem involves a mix of Python review (loops, conditionals, counters/dictionaries, sorting) and learning to work with a sizable collection of text data.\n",
    "\n",
    "You will be looking at a phenomenon of \"natural languages\" (i.e., human languages, such as English) called *Zipf's law*, which relates how frequently a word occurs in a language to its \"rank\" (the word with rank 1 is the most frequently occurring word, the word with rank 2 is the second most frequently occurring word, etc). Roughly speaking, the word with rank 1 appears twice as likely as the word with rank 2, and the three times as likely as the word with rank 3, and so forth. (Some details on Zipf's law can be found on [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law).) In this problem, you are going to check whether Zipf's law holds for a real dataset of text documents.\n",
    "\n",
    "The dataset we look at is a collection of the 100 most popular books downloaded from the [Gutenburg Project](https://www.gutenberg.org/browse/scores/top). These 100 books form the corpus that we consider for this problem. Each file contains the text of a book. We will read in all 100 books.\n",
    "\n",
    "Note: Please *do not* change the folder name or the path, and make sure you use a relative path (e.g. './HW1_Dataset/*filename*) when reading the files. When grading your homework, we will put your Jupyter notebook file and the dataset in the same folder, and run your code. **You will not receive points for this problem if your code fails to load the data.**\n",
    "\n",
    "Hint: To list all files that match a certain pattern, you can use the `glob` package. Here's an example usage:\n",
    "\n",
    "```python\n",
    "import glob\n",
    "print(glob.glob('./HW1_Dataset/*.txt'))\n",
    "```\n",
    "\n",
    "**(a) Warm-up/basic Python review [15 pts across subparts].** This part serves as a warm-up, getting you familiar with the kind of code we will be writing in this class. Note that throughout part (a), your code should **not** be using spaCy.\n",
    "\n",
    "**Subpart i [5 pts].** Write a loop that iterates through all 100 books; for each book, print out its corresponding file name and also how long the book is in terms of string length (meaning that if we load in the book as a string, we compute the length of the string using the built-in Python function `len`; this is just counting the number of characters).\n",
    "\n",
    "**Please do not actually print out the contents of each book since many of the books are extremely long, and by printing out all the books' contents, you'll end up creating a Jupyter notebook that has a massive file size.**\n",
    "\n",
    "Hint: When debugging your code, you may want to first make sure your code runs on a few of the books rather than all 100 (for example, you can start by only having 3 of the text files in `HW1_Dataset`). Once you're confident that your solution is correct on a few text files, then run on all of them! This is a standard approach to debugging code that is meant to handle large datasets.\n",
    "\n",
    "Your output should look like (although there should be 100 books rather than 3 as shown below; also, the ordering of the books might be different on your machine):\n",
    "\n",
    "```\n",
    "./HW1_Dataset/War and Peace by graf Leo Tolstoy (251).txt 3227580\n",
    "./HW1_Dataset/Democracy in America — Volume 1 by Alexis de Tocqueville (147).txt 1148435\n",
    "./HW1_Dataset/Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (501).txt 441034\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UgLpkB5_IUhp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./HW1_Dataset/War and Peace by graf Leo Tolstoy (251).txt 3227580\n",
      "./HW1_Dataset/Democracy in America — Volume 1 by Alexis de Tocqueville (147).txt 1148435\n",
      "./HW1_Dataset/Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (501).txt 441034\n",
      "./HW1_Dataset/The King James Version of the Bible (132).txt 4351846\n",
      "./HW1_Dataset/Et dukkehjem. English by Henrik Ibsen (557).txt 161241\n",
      "./HW1_Dataset/Outpost in the Wilderness_ Fort Wayne, 1706-1828 by Charles Poinsatte (161).txt 363234\n",
      "./HW1_Dataset/The Romance of Lust_ A Classic Victorian erotic novel by Anonymous (294).txt 1051148\n",
      "./HW1_Dataset/The Odyssey by Homer (124).txt 697404\n",
      "./HW1_Dataset/A History of Spanish Literature by James Fitzmaurice-Kelly (109).txt 765918\n",
      "./HW1_Dataset/The Philosophy of Spiritual Activity by Rudolf Steiner (144).txt 545031\n",
      "./HW1_Dataset/My Fifteen Lost Years by Florence Elizabeth Maybrick (499).txt 383694\n",
      "./HW1_Dataset/Prestuplenie i nakazanie. English by Fyodor Dostoyevsky (160).txt 1154508\n",
      "./HW1_Dataset/The History of the Peloponnesian War by Thucydides (122).txt 1200021\n",
      "./HW1_Dataset/Gulliver's Travels into Several Remote Nations of the World by Jonathan Swift (136).txt 600105\n",
      "./HW1_Dataset/The Peddler Spy by W. J. Hamilton (356).txt 197390\n",
      "./HW1_Dataset/Dubliners by James Joyce (219).txt 389199\n",
      "./HW1_Dataset/The Hound of the Baskervilles by Arthur Conan Doyle (223).txt 339188\n",
      "./HW1_Dataset/The Boy Fortune Hunters in China by L. Frank  Baum (215).txt 324452\n",
      "./HW1_Dataset/Leviathan by Thomas Hobbes (417).txt 1231784\n",
      "./HW1_Dataset/The Awakening, and Selected Short Stories by Kate Chopin (192).txt 378936\n",
      "./HW1_Dataset/The Interesting Narrative of the Life of Olaudah Equiano, Or Gustavus Vassa, The African by Equiano (114).txt 471165\n",
      "./HW1_Dataset/Through the Looking-Glass by Lewis Carroll (134).txt 181667\n",
      "./HW1_Dataset/Il Principe. English by Niccolò Machiavelli (428) (Unicode Encoding Conflict).txt 300788\n",
      "./HW1_Dataset/On Liberty by John Stuart Mill (116).txt 325922\n",
      "./HW1_Dataset/The Law of Storms by John Wilson Ross (110).txt 65264\n",
      "./HW1_Dataset/The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson (170).txt 159542\n",
      "./HW1_Dataset/Liberalism by L. T.  Hobhouse (119).txt 305345\n",
      "./HW1_Dataset/Semmering 1912 by Peter Altenberg (221).txt 264946\n",
      "./HW1_Dataset/Les Misérables by Victor Hugo (165) (Unicode Encoding Conflict).txt 3254333\n",
      "./HW1_Dataset/The Count of Monte Cristo, Illustrated by Alexandre Dumas (160).txt 2643849\n",
      "./HW1_Dataset/Alice in Wonderland by Lewis Carroll and Alice Gerstenberg (126).txt 104079\n",
      "./HW1_Dataset/Common Sense by Thomas Paine (297).txt 145353\n",
      "./HW1_Dataset/Narrative of the Captivity and Restoration of Mrs. Mary Rowlandson by Mary White Rowlandson (222).txt 123784\n",
      "./HW1_Dataset/The Divine Comedy by Dante, Illustrated by Dante Alighieri (119).txt 2161\n",
      "./HW1_Dataset/The Importance of Being Earnest_ A Trivial Comedy for Serious People by Oscar Wilde (454).txt 138887\n",
      "./HW1_Dataset/Walden, and On The Duty Of Civil Disobedience by Henry David Thoreau (180).txt 656207\n",
      "./HW1_Dataset/The Yellow Wallpaper by Charlotte Perkins Gilman (462).txt 50841\n",
      "./HW1_Dataset/Jane Eyre_ An Autobiography by Charlotte Brontë (290) (Unicode Encoding Conflict).txt 1049267\n",
      "./HW1_Dataset/The Kama Sutra of Vatsyayana by Vatsyayana (172).txt 351664\n",
      "./HW1_Dataset/Sense and Sensibility by Jane Austen (177).txt 693095\n",
      "./HW1_Dataset/Adventures of Huckleberry Finn by Mark Twain (401).txt 593962\n",
      "./HW1_Dataset/Heart of Darkness by Joseph Conrad (213).txt 229835\n",
      "./HW1_Dataset/The Republic by Plato (246).txt 1214387\n",
      "./HW1_Dataset/The Complete Works of William Shakespeare by William Shakespeare (132).txt 5465100\n",
      "./HW1_Dataset/The Picture of Dorian Gray by Oscar Wilde (282).txt 453169\n",
      "./HW1_Dataset/The Works of Edgar Allan Poe — Volume 2 by Edgar Allan Poe (137).txt 569966\n",
      "./HW1_Dataset/Ulysses by James Joyce (282).txt 1534140\n",
      "./HW1_Dataset/The Federalist Papers by Alexander Hamilton and John Jay and James Madison (116).txt 1167141\n",
      "./HW1_Dataset/Leaves of Grass by Walt Whitman (164).txt 757361\n",
      "./HW1_Dataset/Narrative of the Life of Frederick Douglass, an American Slave by Frederick Douglass (243).txt 244263\n",
      "./HW1_Dataset/A Study in Scarlet by Arthur Conan Doyle (163).txt 262340\n",
      "./HW1_Dataset/Beowulf An Anglo-Saxon Epic Poem (403).txt 292953\n",
      "./HW1_Dataset/The Wendigo by Algernon Blackwood (174).txt 127464\n",
      "./HW1_Dataset/The Scarlet Letter by Nathaniel Hawthorne (211).txt 506975\n",
      "./HW1_Dataset/Persuasion by Jane Austen (2234).txt 486251\n",
      "./HW1_Dataset/The Time Machine by H. G.  Wells (137).txt 198281\n",
      "./HW1_Dataset/The Guardsman by Homer Greene (175).txt 412900\n",
      "./HW1_Dataset/Carmilla by Joseph Sheridan Le Fanu (135).txt 176408\n",
      "./HW1_Dataset/Uncle Tom's Cabin by Harriet Beecher Stowe (108).txt 1025501\n",
      "./HW1_Dataset/A Tale of Two Cities by Charles Dickens (425).txt 776702\n",
      "./HW1_Dataset/Moby Dick; Or, The Whale by Herman Melville (413).txt 1238569\n",
      "./HW1_Dataset/Songs of Innocence, and Songs of Experience by William Blake (166).txt 51724\n",
      "./HW1_Dataset/Dracula by Bram Stoker (461).txt 867141\n",
      "./HW1_Dataset/Emma by Jane Austen (166).txt 902390\n",
      "./HW1_Dataset/Wuthering Heights by Emily Brontë (179) (Unicode Encoding Conflict).txt 669150\n",
      "./HW1_Dataset/Autobiography of Benjamin Franklin by Benjamin Franklin (247).txt 458600\n",
      "./HW1_Dataset/The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson (158).txt 158048\n",
      "./HW1_Dataset/Anne of Green Gables by L. M.  Montgomery (114).txt 580635\n",
      "./HW1_Dataset/The Souls of Black Folk by W. E. B.  Du Bois (172).txt 421336\n",
      "./HW1_Dataset/The Confessions of St. Augustine by Bishop of Hippo Saint Augustine (157).txt 622405\n",
      "./HW1_Dataset/Candide by Voltaire (180).txt 224924\n",
      "./HW1_Dataset/The Legend of Sleepy Hollow by Washington Irving (399).txt 89237\n",
      "./HW1_Dataset/Alice's Adventures in Wonderland by Lewis Carroll (564).txt 163817\n",
      "./HW1_Dataset/Peter Pan by J. M.  Barrie (274).txt 278194\n",
      "./HW1_Dataset/The Wonderful Wizard of Oz by L. Frank  Baum (150).txt 227654\n",
      "./HW1_Dataset/The Innocents Abroad by Mark Twain (108).txt 1126278\n",
      "./HW1_Dataset/The Turn of the Screw by Henry James (185).txt 248089\n",
      "./HW1_Dataset/The Adventures of Tom Sawyer by Mark Twain (371).txt 412712\n",
      "./HW1_Dataset/The Tragedy of Romeo and Juliet by William Shakespeare (131).txt 174128\n",
      "./HW1_Dataset/A Modest Proposal by Jonathan Swift (233).txt 38966\n",
      "./HW1_Dataset/A Christmas Carol in Prose; Being a Ghost Story of Christmas by Charles Dickens (265).txt 177758\n",
      "./HW1_Dataset/Essays of Michel de Montaigne — Complete by Michel de Montaigne (205).txt 3033575\n",
      "./HW1_Dataset/The Adventures of Sherlock Holmes by Arthur Conan Doyle (447).txt 581864\n",
      "./HW1_Dataset/Robert Greene by Robert Greene (126).txt 760655\n",
      "./HW1_Dataset/Essays by Ralph Waldo Emerson by Ralph Waldo Emerson (239).txt 557197\n",
      "./HW1_Dataset/The Boy Fortune Hunters in the South Seas by L. Frank  Baum (150).txt 266166\n",
      "./HW1_Dataset/Pride and Prejudice by Jane Austen (1003).txt 704192\n",
      "./HW1_Dataset/My Secret Life, Volumes I. to III. by Anonymous (157).txt 1378627\n",
      "./HW1_Dataset/The Mysterious Affair at Styles by Agatha Christie (115).txt 340770\n",
      "./HW1_Dataset/Treasure Island by Robert Louis Stevenson (236).txt 383675\n",
      "./HW1_Dataset/Hedda Gabler by Henrik Ibsen (123).txt 193875\n",
      "./HW1_Dataset/Metamorphosis by Franz Kafka (483).txt 139056\n",
      "./HW1_Dataset/Great Expectations by Charles Dickens (293).txt 1013450\n",
      "./HW1_Dataset/The Life and Adventures of Robinson Crusoe by Daniel Defoe (126).txt 642909\n",
      "./HW1_Dataset/The Iliad by Homer (256).txt 1175588\n",
      "./HW1_Dataset/The Tragical History of Doctor Faustus by Christopher Marlowe (144).txt 143736\n",
      "./HW1_Dataset/Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm (429).txt 540241\n",
      "./HW1_Dataset/Beyond Good and Evil by Friedrich Wilhelm Nietzsche (120).txt 402273\n",
      "./HW1_Dataset/Siddhartha by Hermann Hesse (216).txt 236855\n",
      "./HW1_Dataset/The Faery Queen and Her Knights by Alfred John Church (177).txt 447320\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "filelist = glob.glob('./HW1_Dataset/*.txt')\n",
    "for file in filelist:\n",
    "    with open(file, 'r') as f:\n",
    "        data=f.read()\n",
    "        print(file,len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efsp_yqzIUho"
   },
   "source": [
    "**Subpart ii [5 pts].** Copy and paste your code from **subpart i** into the code cell below. Then in the code cell below, modify the code so that it prints out the top 15 longest books (in terms of raw string length, which is what you had already computed). In particular, please write your code so that the printout is of the following format:\n",
    "\n",
    "```\n",
    "1. <number of characters in longest text file> <filename of longest text file>\n",
    "2. <number of characters in 2nd longest text file> <filename of 2nd longest text file>\n",
    "...\n",
    "15. <number of characters in 15th longest text file> <filename of 15th longest text file>\n",
    "```\n",
    "\n",
    "Note: only print out information for the top 15 books in the format above; please do not repeat printing what we asked you to print in **subpart i**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465100 ./HW1_Dataset/The Complete Works of William Shakespeare by William Shakespeare (132).txt\n",
      "4351846 ./HW1_Dataset/The King James Version of the Bible (132).txt\n",
      "3254333 ./HW1_Dataset/Les Misérables by Victor Hugo (165) (Unicode Encoding Conflict).txt\n",
      "3227580 ./HW1_Dataset/War and Peace by graf Leo Tolstoy (251).txt\n",
      "3033575 ./HW1_Dataset/Essays of Michel de Montaigne — Complete by Michel de Montaigne (205).txt\n",
      "2643849 ./HW1_Dataset/The Count of Monte Cristo, Illustrated by Alexandre Dumas (160).txt\n",
      "1534140 ./HW1_Dataset/Ulysses by James Joyce (282).txt\n",
      "1378627 ./HW1_Dataset/My Secret Life, Volumes I. to III. by Anonymous (157).txt\n",
      "1238569 ./HW1_Dataset/Moby Dick; Or, The Whale by Herman Melville (413).txt\n",
      "1231784 ./HW1_Dataset/Leviathan by Thomas Hobbes (417).txt\n",
      "1214387 ./HW1_Dataset/The Republic by Plato (246).txt\n",
      "1200021 ./HW1_Dataset/The History of the Peloponnesian War by Thucydides (122).txt\n",
      "1175588 ./HW1_Dataset/The Iliad by Homer (256).txt\n",
      "1167141 ./HW1_Dataset/The Federalist Papers by Alexander Hamilton and John Jay and James Madison (116).txt\n",
      "1154508 ./HW1_Dataset/Prestuplenie i nakazanie. English by Fyodor Dostoyevsky (160).txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "dict = {}\n",
    "for name in glob.glob('./HW1_Dataset/*.txt'):\n",
    "    with open(name, \"r\") as text:\n",
    "        textfile = text.read()\n",
    "    dict[len(textfile)] = name\n",
    "\n",
    "for i in sorted(dict.keys(), reverse = True)[:15]:\n",
    "    print(i, dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nFDZhmjIUhp"
   },
   "source": [
    "**Subpart iii [5 pts].** You should find that some of the books are quite long. Processing very long books will be problematic with spaCy, so we will want to chop up long books into pieces. Note that later on in the course, we shall see that chopping up large amounts of data into small pieces or \"batches\" is in fact quite common in machine learning since for massive datasets, we often cannot store them in their entirety on a CPU or GPU for processing all at once.\n",
    "\n",
    "As a toy example, suppose that a book's text is `'cat dog shark spam eggs'` and we want to split it up into batches where each batch has at most 3 words. Then we could split up the book's text into two batches/pieces: `'cat dog shark'` and `'spam eggs'`.\n",
    "\n",
    "We have provided code for you to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yZV2gyhxIUhp",
    "outputId": "0d05f142-d05b-4cba-efa6-32f794b740c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat dog shark', 'spam eggs']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_into_batches(book_text, max_num_words_per_batch):\n",
    "    words_split_on_spaces = book_text.split(' ')\n",
    "    num_book_pieces = int(np.ceil(len(words_split_on_spaces) / max_num_words_per_batch))\n",
    "    batches = []\n",
    "    for piece_idx in range(num_book_pieces):\n",
    "        start_idx = piece_idx * max_num_words_per_batch\n",
    "        end_idx = (piece_idx + 1) * max_num_words_per_batch\n",
    "        if end_idx > len(words_split_on_spaces):\n",
    "            end_idx = len(words_split_on_spaces)\n",
    "        book_piece = ' '.join(words_split_on_spaces[start_idx:end_idx])\n",
    "        batches.append(book_piece)\n",
    "    return batches\n",
    "\n",
    "print(split_into_batches('cat dog shark spam eggs', 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wjar87AIUhq"
   },
   "source": [
    "Now write a loop that goes through all 100 books again. However, for each book, split it up into batches so that each batch has at most 10000 words. In this case, how many batches are there total across all 100 books? Please write code that computes this number of batches. Your code should print out the following (with the correct total number of batches):\n",
    "\n",
    "```\n",
    "Total number of batches: <total number of batches across all 100 books>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "K6k24KIUIUhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of batches:  1246\n"
     ]
    }
   ],
   "source": [
    "batch=[]\n",
    "for file in filelist:\n",
    "    with open(file, 'r') as f:\n",
    "        data=f.read()\n",
    "        a2=len(split_into_batches(data,10000))\n",
    "        batch.append(a2)\n",
    "        \n",
    "print(\"Total Number of batches: \", sum(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFuaYyVYIUhr"
   },
   "source": [
    "**(b) [10 pts]** Now we will finally use spaCy. Note that for this problem, you don't actually need `spaCy`'s named entity recognition or grammatical parsing. Turning these elements off when you instantiate the `nlp` object can substantially speed up your code. To make sure these are off when instantiating the `nlp` object, call: \n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "```\n",
    "\n",
    "**Throughout this problem, do not remove stopwords.**\n",
    "\n",
    "Build a term frequency (raw counts) table that is for all 100 books. Specifically, building on your solution to part (a)-subpart iii, read in a single book at a time and for each book, do the following:\n",
    "\n",
    "  1. Split it up into batches where each batch has at most 10000 words.\n",
    "  2. For each batch:\n",
    "    1. Process the batch by separating and lemmatizing the words\n",
    "    2. Count the number of times each lemma appears and add these to the frequency table. For simplicity, **do not convert lemmas to lowercase**. (Note that we use raw counts as the \"frequency\"--do not do any division.) Also, as we explain below, we will only count lemmas that are alphabetic.\n",
    "\n",
    "Note that just as we had said in part (a): do **not** print out the complete contents of every book since doing so will result in a Jupyter notebook file that is massive.\n",
    "\n",
    "After looping through all 100 books, you should have the term frequency table for the entire corpus (importantly, the frequency table should not just be for a single book; it should be for all 100 books). Sort the table and print the top 50 most frequent words, along with their frequencies and ranks. Don't worry about ties (for example, if multiple things have the same frequency, it's fine if your solution breaks ties arbitrarily in the sorting).\n",
    "\n",
    "Note: When counting the lemmas, only include lemmas that consist of alphabetic letters (a-z and A-Z). You can do this with what's called a *regular expression*. For example, to check whether the words \"will.i.am\" or \"Tesla\" are alphabetic, you would do the following:\n",
    "\n",
    "```python\n",
    "import re  # regular expression package\n",
    "if re.match('[a-zA-Z]+$', 'will.i.am'):\n",
    "    print('will.i.am consists only of alphabetic letters!')\n",
    "if re.match('[a-zA-Z]+$', 'Tesla'):\n",
    "    print('tesla consists only of alphabetic letters!')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/vibhasgoel/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zigJ5re8IUhr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>702465</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be</td>\n",
       "      <td>436114</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>435424</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>386347</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>324409</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>255129</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>233423</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>204252</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he</td>\n",
       "      <td>193622</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>159010</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>158132</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>135431</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>not</td>\n",
       "      <td>111854</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>108536</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you</td>\n",
       "      <td>102595</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>with</td>\n",
       "      <td>101227</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>they</td>\n",
       "      <td>96821</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>for</td>\n",
       "      <td>95320</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>as</td>\n",
       "      <td>90702</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>she</td>\n",
       "      <td>78524</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>but</td>\n",
       "      <td>75599</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>my</td>\n",
       "      <td>73983</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>do</td>\n",
       "      <td>68754</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>at</td>\n",
       "      <td>64947</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>on</td>\n",
       "      <td>62833</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>by</td>\n",
       "      <td>61274</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>this</td>\n",
       "      <td>61240</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>we</td>\n",
       "      <td>60300</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>which</td>\n",
       "      <td>58868</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>all</td>\n",
       "      <td>58636</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>say</td>\n",
       "      <td>58348</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>from</td>\n",
       "      <td>50095</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>so</td>\n",
       "      <td>47999</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>or</td>\n",
       "      <td>47810</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>her</td>\n",
       "      <td>42838</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>one</td>\n",
       "      <td>41123</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>will</td>\n",
       "      <td>40920</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>man</td>\n",
       "      <td>39585</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>their</td>\n",
       "      <td>38418</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>what</td>\n",
       "      <td>37851</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>no</td>\n",
       "      <td>37808</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>there</td>\n",
       "      <td>37209</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>if</td>\n",
       "      <td>34898</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>go</td>\n",
       "      <td>33897</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>when</td>\n",
       "      <td>33587</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>an</td>\n",
       "      <td>32517</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>come</td>\n",
       "      <td>32358</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>see</td>\n",
       "      <td>32024</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>would</td>\n",
       "      <td>31289</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>who</td>\n",
       "      <td>30934</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Frequency  Rank\n",
       "0     the     702465   1.0\n",
       "1      be     436114   2.0\n",
       "2     and     435424   3.0\n",
       "3      of     386347   4.0\n",
       "4      to     324409   5.0\n",
       "5       I     255129   6.0\n",
       "6       a     233423   7.0\n",
       "7      in     204252   8.0\n",
       "8      he     193622   9.0\n",
       "9    that     159010  10.0\n",
       "10   have     158132  11.0\n",
       "11     it     135431  12.0\n",
       "12    not     111854  13.0\n",
       "13    his     108536  14.0\n",
       "14    you     102595  15.0\n",
       "15   with     101227  16.0\n",
       "16   they      96821  17.0\n",
       "17    for      95320  18.0\n",
       "18     as      90702  19.0\n",
       "19    she      78524  20.0\n",
       "20    but      75599  21.0\n",
       "21     my      73983  22.0\n",
       "22     do      68754  23.0\n",
       "23     at      64947  24.0\n",
       "24     on      62833  25.0\n",
       "25     by      61274  26.0\n",
       "26   this      61240  27.0\n",
       "27     we      60300  28.0\n",
       "28  which      58868  29.0\n",
       "29    all      58636  30.0\n",
       "30    say      58348  31.0\n",
       "31   from      50095  32.0\n",
       "32     so      47999  33.0\n",
       "33     or      47810  34.0\n",
       "34    her      42838  35.0\n",
       "35    one      41123  36.0\n",
       "36   will      40920  37.0\n",
       "37    man      39585  38.0\n",
       "38  their      38418  39.0\n",
       "39   what      37851  40.0\n",
       "40     no      37808  41.0\n",
       "41  there      37209  42.0\n",
       "42     if      34898  43.0\n",
       "43     go      33897  44.0\n",
       "44   when      33587  45.0\n",
       "45     an      32517  46.0\n",
       "46   come      32358  47.0\n",
       "47    see      32024  48.0\n",
       "48  would      31289  49.0\n",
       "49    who      30934  50.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "list = []\n",
    "histogram = {}\n",
    "for name in glob.glob('./HW1_Dataset/*.txt'):\n",
    "    with open(name, \"r\") as text:\n",
    "        textfile = text.read()\n",
    "    for lname in split_into_batches(textfile, 10000):\n",
    "        for token in nlp(lname):\n",
    "            if re.match('[a-zA-Z]+$', token.lemma_):\n",
    "                if token.lemma_ not in histogram:  \n",
    "                    histogram[token.lemma_] = 1\n",
    "                else:  \n",
    "                    histogram[token.lemma_] += 1\n",
    "\n",
    "sorted_lemma_count_pairs = sorted(histogram.items(),\n",
    "                                  reverse=True,\n",
    "                                  key=itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(sorted_lemma_count_pairs)\n",
    "df.columns = ['Word', 'Frequency']\n",
    "\n",
    "df[\"Rank\"] = df[\"Frequency\"].rank(ascending = False)\n",
    "df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your 5\n",
      "you 20\n",
      "years 1\n",
      "written 3\n",
      "writing 3\n",
      "works 12\n",
      "work 14\n",
      "without 2\n",
      "within 4\n",
      "with 16\n",
      "wish 1\n",
      "will 1\n",
      "widest 2\n",
      "wide 1\n",
      "who 2\n",
      "which 5\n",
      "where 2\n",
      "web 1\n",
      "we 6\n",
      "ways 1\n",
      "was 2\n",
      "warranties 1\n",
      "walks 1\n",
      "volunteers 5\n",
      "volunteer 1\n",
      "void 1\n",
      "visit 2\n",
      "virus 1\n",
      "violates 1\n",
      "variety 1\n",
      "user 2\n",
      "use 1\n",
      "us 1\n",
      "upon 1\n",
      "up 2\n",
      "unsolicited 1\n",
      "unless 1\n",
      "uniform 1\n",
      "unenforceability 1\n",
      "under 3\n",
      "types 1\n",
      "treatment 1\n",
      "transcription 1\n",
      "transcribe 1\n",
      "trademark 4\n",
      "to 46\n",
      "tm 25\n"
     ]
    }
   ],
   "source": [
    "histogram = {}\n",
    "for token in parsed_text:\n",
    "    original_token_text = token.orth_  # `token` itself is not a string; `token.orth_` gives the string representation of `token`\n",
    "    if original_token_text not in histogram:  # if the token text is not already a key in the dictionary, create it and set its value to 1\n",
    "        histogram[original_token_text] = 1\n",
    "    else:  # the token text is already a key in the dictionary so increase its value by 1\n",
    "        histogram[original_token_text] += 1\n",
    "\n",
    "import re  # regular expression package\n",
    "for i in sorted(histogram, reverse = True)[:50]:\n",
    "    if re.match('[a-zA-Z]+$', i):\n",
    "        print(i, histogram[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1=[]\n",
    "for file in filelist:\n",
    "    with open(file, 'r') as f:\n",
    "        data=f.read()\n",
    "        a2=split_into_batches(data,10000)\n",
    "        for i in range (0,len(a2)):\n",
    "            book1.append(a2[i])\n",
    "        \n",
    "histogram = {}      \n",
    "#print(len(book1))\n",
    "for i in book1:\n",
    "    parsed_text=nlp(i)\n",
    " \n",
    "    for token in parsed_text:\n",
    "        original_token_text = token.orth_  # `token` itself is not a string; `token.orth_` gives the string representation of `token`\n",
    "        if original_token_text not in histogram:  # if the token text is not already a key in the dictionary, create it and set its value to 1\n",
    "            histogram[original_token_text] = 1\n",
    "        else:  # the token text is already a key in the dictionary so increase its value by 1\n",
    "            histogram[original_token_text] += 1\n",
    "    \n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGpRkIBFIUhr"
   },
   "source": [
    "**(c) [10 pts]** Visualize the frequency table by plotting a **raw scatter plot** (put frequency as the y-axis and rank as the x-axis), and a **log-log plot** (use logarithmic scales on both the x- and y- axes). Note that this should be for all words and not only the top 50. As before, for the ranks, do not worry about ties, i.e., break ties arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o70-u378IUhr"
   },
   "outputs": [],
   "source": [
    "# Raw scatter plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toULd9vUIUhr"
   },
   "outputs": [],
   "source": [
    "# Log-log plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDfuokiVIUhr"
   },
   "source": [
    "**(d) [10 pts across subparts]** Let's now try to make sense of the very last plot in part **(c)**. Zipf's law states that term frequency is governed by a power law, i.e. the relationship between term frequency and rank can be approximated by $f(r) = cr^{-1}$, where $f(r)$ is the frequency of the term at rank $r$, $r$ is the rank of a term, and $c$ is a constant that is approximately 0.1*(corpus size) for English.\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "**Subpart i [3 pts].** What do you observe in the log-log plot above? Is this consist with the power law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuxRv41gIUhr"
   },
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGE9NFmKIUhs"
   },
   "source": [
    "**Subpart ii [4 pts].** Think of the corpus as a (large) unigram bag of words. Following the analogy from lecture, imagine drawing a single word from this big bag (note that we are assuming that we've lemmatized the words and also filtered out non-alphabetic words; thus what remains in the bag are actually alphabetic lemmas). What is the probability of drawing one of the 4 most frequent alphabetic lemmas? What is the probability of drawing one of the 50 most frequent alphabetic lemmas? Answer these two questions using code rather than just entering in the final answers as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTc9t8OhIUhs",
    "outputId": "84eb8f27-93d8-4228-e93a-4e518558e655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of drawing one of the 4 most frequent alphabetic lemmas: \n",
      "Probability of drawing one of the 50 most frequent alphabetic lemmas: \n"
     ]
    }
   ],
   "source": [
    "print('Probability of drawing one of the 4 most frequent alphabetic lemmas: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "\n",
    "print('Probability of drawing one of the 50 most frequent alphabetic lemmas: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59gHjrftIUhs"
   },
   "source": [
    "**Subpart iii [3 pts].** What proportion of the alphabetic lemmas occur only once? What proportion of the alphabetic lemmas occur fewer than 10 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbyM-TU2IUhs",
    "outputId": "77d23ea6-cf4e-49a8-91a9-e80ad344706a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occur only once: \n",
      "Occur fewer than 10 times: \n"
     ]
    }
   ],
   "source": [
    "print(\"Occur only once: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "\n",
    "print(\"Occur fewer than 10 times: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I63qS2ROIUhs"
   },
   "source": [
    "### [Problem 2] Entity Recognition and Pointwise Mutual Information (PMI) [55 pts]\n",
    "By using the entity recognition system in `spaCy`, let's identify named entities from newspaper articles. You'll be using Reuters corpus which contains more than ten thousand newspaper articles. To run the code below, you need to download the Reuters dataset. To do so, in a terminal/command line (recall that you can open a terminal from Jupyter's webpage that shows all the files, which by default is [http://localhost:8888/tree](http://localhost:8888/tree)), start up Python and enter:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "```\n",
    "\n",
    "Then proceed to the problem subparts below.\n",
    "\n",
    "Note that in this problem you will need named entity recognition but not grammatical parsing. Hence, you will want to instantiate the nlp object by calling:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xs7AaxHIUhs"
   },
   "source": [
    "**(a) [15 pts]** Draw a bar chart in which one of the axes shows entity labels and the other shows the frequency of the corresponding label. Use the variables `reuters_nlp` and `label_counter` provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSZv6xESIqfo",
    "outputId": "ec8765c2-cd4c-4b45-aa15-64ee141bb342"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/vibhasgoel/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gok69eE5IUhs",
    "outputId": "4e2fe383-a0b5-4c78-f2b8-57004dda0c04",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m reuters_fileids \u001b[38;5;241m=\u001b[39m reuters\u001b[38;5;241m.\u001b[39mfileids()  \u001b[38;5;66;03m# hint: when first debugging, consider looking at just the first few\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m reuters_nlp \u001b[38;5;241m=\u001b[39m [nlp(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, reuters\u001b[38;5;241m.\u001b[39mraw(i))\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m reuters_fileids]\n\u001b[1;32m      9\u001b[0m label_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m reuters_nlp:\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m reuters_fileids \u001b[38;5;241m=\u001b[39m reuters\u001b[38;5;241m.\u001b[39mfileids()  \u001b[38;5;66;03m# hint: when first debugging, consider looking at just the first few\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m reuters_nlp \u001b[38;5;241m=\u001b[39m [\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreuters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m reuters_fileids]\n\u001b[1;32m      9\u001b[0m label_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m reuters_nlp:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/ml/parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/with_array.py:38\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/with_array.py:73\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     71\u001b[0m lengths \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     72\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 73\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     76\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/maxout.py:53\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 53\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     55\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger'])\n",
    "reuters_fileids = reuters.fileids()  # hint: when first debugging, consider looking at just the first few\n",
    "reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i)).strip()) for i in reuters_fileids]\n",
    "label_counter = Counter()\n",
    "\n",
    "for doc in reuters_nlp:\n",
    "    for ent in doc.ents:\n",
    "        label_counter[ent.label_] +=1\n",
    "\n",
    "entity_labels = list(label_counter.keys())\n",
    "frequencies = list(label_counter.values())\n",
    "\n",
    "plt.bar(entity_labels, frequencies)\n",
    "plt.xlabel('Entity Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Entity Labels in the Reuters Corpus')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2UQvizFIUhs"
   },
   "source": [
    "**(b) [20 pts]** Now list the top 10 most frequently occurring entities (entity text and the number of occurence) with label `ORG` (organization). Separately list the top 10 most frequently occurring entities with label `GPE` (geopolitical entity such as countries, cities, states) respectively. **In both cases, please convert the entity names to lowercase first before computing the top 10.**\n",
    "\n",
    "Here, when counting the (raw count) frequency, we need to count how many articles have an entity with the desired property. For every article, we add 1 if the article has the entity and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0aRfGwKbIUht"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequently Occurring Entities with Label ORG:\n",
      "cts 2220\n",
      "ec 799\n",
      "fed 653\n",
      "reuters 505\n",
      "opec 499\n",
      "treasury 341\n",
      "usda 339\n",
      "bundesbank 314\n",
      "the securities and exchange commission 252\n",
      "dlrs 220\n",
      "Top 10 Most Frequently Occurring Entities with Label GPE:\n",
      "u.s. 3910\n",
      "japan 1483\n",
      "the united states 592\n",
      "brazil 500\n",
      "canada 436\n",
      "u.k. 402\n",
      "china 337\n",
      "paris 336\n",
      "washington 330\n",
      "taiwan 318\n"
     ]
    }
   ],
   "source": [
    "org_list = []\n",
    "gpe_list = []\n",
    "\n",
    "for doc in reuters_nlp:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "            org_list.append(ent.text.lower())\n",
    "        if ent.label_ == 'GPE':\n",
    "            gpe_list.append(ent.text.lower())\n",
    "\n",
    "org_counter = Counter(org_list)\n",
    "gpe_counter = Counter(gpe_list)\n",
    "\n",
    "top_10_org = org_counter.most_common(10)\n",
    "top_10_gpe = gpe_counter.most_common(10)\n",
    "\n",
    "print(\"Top 10 Most Frequently Occurring Entities with Label ORG:\")\n",
    "for entity, count in top_10_org:\n",
    "    print(entity, count)\n",
    "    \n",
    "print(\"Top 10 Most Frequently Occurring Entities with Label GPE:\")\n",
    "for entity, count in top_10_gpe:\n",
    "    print(entity, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ORGs:  [('cts', 2220), ('ec', 799), ('fed', 653), ('reuters', 505), ('opec', 499), ('treasury', 341), ('usda', 339), ('bundesbank', 314), ('the securities and exchange commission', 252), ('dlrs', 220)]\n",
      "Top 10 GPEs:  [('u.s.', 3910), ('japan', 1483), ('the united states', 592), ('brazil', 500), ('canada', 436), ('u.k.', 402), ('china', 337), ('paris', 336), ('washington', 330), ('taiwan', 318)]\n"
     ]
    }
   ],
   "source": [
    "org_entities = []\n",
    "for i in reuters_nlp:\n",
    "  for j in i.ents:\n",
    "    if j.label_ == 'ORG':\n",
    "      org_entities.append(j.text.lower())\n",
    "org_counter = Counter(org_entities)\n",
    "top_10_orgs = org_counter.most_common(10)\n",
    "gpe_entities = []\n",
    "for i in reuters_nlp:\n",
    "  for j in i.ents:\n",
    "    if j.label_ == 'GPE':\n",
    "      gpe_entities.append(j.text.lower())\n",
    "gpe_counter = Counter(gpe_entities)\n",
    "top_10_gpes = gpe_counter.most_common(10)\n",
    "print(\"Top 10 ORGs: \", top_10_orgs)\n",
    "print(\"Top 10 GPEs: \", top_10_gpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awogDo8qIUht"
   },
   "source": [
    "**(c) [20 pts]** Give the top 50 `GPE` (countries, cities, states) entities that have the highest Pointwise Mutual Information (PMI) values with regard to the `ORG` (organization) entity **'opec'** (your list of this top 50 should be ranked in decreasing PMI value). Did you find any unexpected results? If so, why do you think it happened? If you found some of the results to be unsurprisingly, how come? **Just like in the previous part, please convert entity names to lowercase in your analysis.**\n",
    "\n",
    "Hint 1: As in lecture, when computing PMI, we will compute probabilities by counting the number of documents where entities occur or co-occur.  For example, $P('opec') = \\frac{number \\ \\ of \\ \\ documents \\ \\ containing \\ \\ 'opec'}{number \\ \\ of \\ \\ documents}$.  \n",
    "\n",
    "Hint 2: To compute this ranking, you do not have to compute the full PMI equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQhlS9DEIUht"
   },
   "outputs": [],
   "source": [
    "all_pairs = [(gpe, org) for gpe in gpe_list for org in org_list]\n",
    "\n",
    "from collections import Counter\n",
    "co_occurrence_probabilities = Counter()\n",
    "for gpe, org in all_pairs:\n",
    "    count = 0\n",
    "    for doc in reuters_nlp:\n",
    "        if gpe in doc and org in doc:\n",
    "            count += 1\n",
    "    co_occurrence_probabilities[(gpe, org)] = count / len(reuters_nlp)\n",
    "    \n",
    "from math import log  # natural log\n",
    "pmi_scores = Counter()\n",
    "for gpe, org in all_pairs:\n",
    "    ratio = co_occurrence_probabilities[(gpe, org)] / (gpe_probabilities[gpe] * org_probabilities[org])\n",
    "    pmi_scores[(person, company)] = log(ratio)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/vibhasgoel/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import ne_chunk\n",
    "from collections import Counter\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_counter = Counter(reuters.words())\n",
    "\n",
    "word1 = 'opec'\n",
    "ne_words = pos_tag(reuters.words())\n",
    "word2s = [word for word, tag in ne_words if tag == 'GPE']\n",
    "print(word2s)\n",
    "pmis = [(w2, word2s.count(w2) / word_counter[word1]) for w2 in word2s if word_counter[word1] > 0]\n",
    "pmis.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_50 = pmis[:50]\n",
    "top_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIkSKMl1IUht"
   },
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
